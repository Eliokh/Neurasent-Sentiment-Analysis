{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "613/613 - 12s - loss: 4.1969 - val_loss: 1.8981 - 12s/epoch - 20ms/step\n",
      "Epoch 2/30\n",
      "613/613 - 10s - loss: 1.8716 - val_loss: 1.8505 - 10s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "613/613 - 9s - loss: 1.8128 - val_loss: 1.7907 - 9s/epoch - 15ms/step\n",
      "Epoch 4/30\n",
      "613/613 - 9s - loss: 1.7225 - val_loss: 1.6827 - 9s/epoch - 14ms/step\n",
      "Epoch 5/30\n",
      "613/613 - 9s - loss: 1.5796 - val_loss: 1.5217 - 9s/epoch - 14ms/step\n",
      "Epoch 6/30\n",
      "613/613 - 8s - loss: 1.3756 - val_loss: 1.3273 - 8s/epoch - 14ms/step\n",
      "Epoch 7/30\n",
      "613/613 - 9s - loss: 1.1972 - val_loss: 1.2023 - 9s/epoch - 14ms/step\n",
      "Epoch 8/30\n",
      "613/613 - 8s - loss: 1.0897 - val_loss: 1.1305 - 8s/epoch - 13ms/step\n",
      "Epoch 9/30\n",
      "613/613 - 10s - loss: 1.0199 - val_loss: 1.0821 - 10s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "613/613 - 12s - loss: 0.9596 - val_loss: 1.0341 - 12s/epoch - 20ms/step\n",
      "Epoch 11/30\n",
      "613/613 - 9s - loss: 0.9090 - val_loss: 0.9917 - 9s/epoch - 15ms/step\n",
      "Epoch 12/30\n",
      "613/613 - 9s - loss: 0.8593 - val_loss: 0.9411 - 9s/epoch - 15ms/step\n",
      "Epoch 13/30\n",
      "613/613 - 9s - loss: 0.8146 - val_loss: 0.9065 - 9s/epoch - 15ms/step\n",
      "Epoch 14/30\n",
      "613/613 - 9s - loss: 0.7763 - val_loss: 0.8762 - 9s/epoch - 14ms/step\n",
      "Epoch 15/30\n",
      "613/613 - 9s - loss: 0.7398 - val_loss: 0.8517 - 9s/epoch - 15ms/step\n",
      "Epoch 16/30\n",
      "613/613 - 9s - loss: 0.7051 - val_loss: 0.8301 - 9s/epoch - 15ms/step\n",
      "Epoch 17/30\n",
      "613/613 - 10s - loss: 0.6761 - val_loss: 0.8164 - 10s/epoch - 16ms/step\n",
      "Epoch 18/30\n",
      "613/613 - 9s - loss: 0.6522 - val_loss: 0.7994 - 9s/epoch - 14ms/step\n",
      "Epoch 19/30\n",
      "613/613 - 10s - loss: 0.6329 - val_loss: 0.7800 - 10s/epoch - 16ms/step\n",
      "Epoch 20/30\n",
      "613/613 - 9s - loss: 0.6082 - val_loss: 0.7705 - 9s/epoch - 15ms/step\n",
      "Epoch 21/30\n",
      "613/613 - 9s - loss: 0.5915 - val_loss: 0.7639 - 9s/epoch - 15ms/step\n",
      "Epoch 22/30\n",
      "613/613 - 9s - loss: 0.5726 - val_loss: 0.7491 - 9s/epoch - 14ms/step\n",
      "Epoch 23/30\n",
      "613/613 - 8s - loss: 0.5523 - val_loss: 0.7657 - 8s/epoch - 12ms/step\n",
      "Epoch 24/30\n",
      "613/613 - 8s - loss: 0.5306 - val_loss: 0.7476 - 8s/epoch - 13ms/step\n",
      "Epoch 25/30\n",
      "613/613 - 9s - loss: 0.5160 - val_loss: 0.7413 - 9s/epoch - 14ms/step\n",
      "Epoch 26/30\n",
      "613/613 - 9s - loss: 0.5024 - val_loss: 0.7260 - 9s/epoch - 15ms/step\n",
      "Epoch 27/30\n",
      "613/613 - 9s - loss: 0.4875 - val_loss: 0.7387 - 9s/epoch - 14ms/step\n",
      "Epoch 28/30\n",
      "613/613 - 11s - loss: 0.4768 - val_loss: 0.7637 - 11s/epoch - 17ms/step\n",
      "Epoch 29/30\n",
      "613/613 - 11s - loss: 0.4641 - val_loss: 0.7398 - 11s/epoch - 18ms/step\n",
      "Epoch 30/30\n",
      "613/613 - 10s - loss: 0.4510 - val_loss: 0.7592 - 10s/epoch - 17ms/step\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.7592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7591838836669922"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import necessary libraries and load the data\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"cleandata.csv\")\n",
    "\n",
    "# 2. Preprocess the data\n",
    "# We'll use a simple Tokenizer to convert our text to numerical data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['Review'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['Review'])\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# 3. Split the data into training and validation sets\n",
    "labels = df['Rating'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Build and train the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=X_train.shape[1]),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(model, new_review):\n",
    "    # Convert the review to the sequence\n",
    "    sequences = tokenizer.texts_to_sequences([new_review])\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=X_train.shape[1])\n",
    "\n",
    "    # Perform the prediction\n",
    "    prediction = model.predict(padded_sequences)\n",
    "\n",
    "    # Return the prediction\n",
    "    return prediction[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "3.3779387\n"
     ]
    }
   ],
   "source": [
    "new_review = \"I hate this tv\"\n",
    "print(predict_review(model, new_review))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
